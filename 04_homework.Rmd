---
title: "SMDS Homework - Block 4"
author: "P. Morichetti, M. Rispoli, A. Cicchini and E. Ballarin  |  Group 'B'"
date: "27th May 2020"
output:
  html_document:
    theme: darkly
 #   highlight: breezedark
    mathjax: default
    self_contained: true
    md_extensions: +autolink_bare_uris
    toc: true
    toc_collapsed: false
    toc_float: true
    toc_depth: 3
    number_sections: false
header-includes:
- \usepackage{color}
- \usepackage{graphicx}
- \usepackage{grffile}
institute: University of Trieste, SISSA, ICTP, University of Udine
graphics: yes
fontsize: 10pt
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/')
```

```{r setup, include=FALSE}
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
```


# Exercises from *DAAG, Chapter 6* 

## Exercise 6

### Text

### Solution

<!-- # Some code... -->
```{r daag_06_06, code = readLines("src/DAAG_06_06.R"), echo=TRUE}
```

### Comments

After having fit both *Model 1* (i.e. the linear model that tries to explain record `time` for male athletes from the `dist`ance and `climb` variables, without considering the interaction term) and *Model 1* (the one considering also interaction between `dist` and `climb`) on the entire, untransformed, `nihills` dataset, we can look at *Fisher-test ANOVA* for model selection, being *Model 1* nested inside *Model 2*.  

With a value of $F = 72.406$ and a *p-value* $<10^{-7}$ *Model 2* is evidently (according to the *F-test*) the model of choice in such setting.  

Upon inspection, however, it appears clear that the relatively poor behaviour of *Model 1* as opposed to *Model 2* is strongly determined by the influence of few data-points.  

In fact, considering for the moment *Model 1*:

- From the *Residuals vs Fitted* graph, it appears clearly that tracks *Seven Sevens* and *Annalong Horseshoe* have both highest residuals (though of opposite sign) and highest fitted values. This evidence of heteroscedasticity is further confirmed by the *Scale-Location* plot.  
- From the *Residuals vs Leverage* plot we know that the *Seven Sevens* track is also a potential influential point.

Now, looking at *Model 2*, we can see that:
- The *Seven Sevens* track has become one of the datapoints with the smallest residuals.  
- Such datapoint still exhibits high leverage and is a candidate to be an influential point.  

We can therefore conclude that the additional interaction term mainly serves the purpose of correcting the model for taking into account the appearantly *strange* behaviour of *Seven Sevens* in *Model 1*.  

We try to re-fit the model on the (still, untransformed) `nihills` dataset, after removal of the *Seven Sevens* datapoint.  

This time, *Fisher-test ANOVA* suggests to avoid the additional interaction term, proving our assumption as probably correct.  

Now, at the price of some additional non-normality even at central quantiles, and without solving the still present problem of heteroscedasticity, the model is less determined by influence of single-datapoints.  

Still, what previously observed for the *Seven Sevens* track can now be found -- even if with a smaller effect overall -- w.r.t. the *Annalong Horseshoe*.  

Repeating the fit with the dataset additionally devoid of the *Annalong Horseshoe* datapoint, here-evidenced conclusions are even more amplified: *Annalong Horseshoe* was an influential point even in the latter case.  

The interesting point to be noted with such regard is that -- being the *Seven Sevens* and *Annalong Horseshoe* datapoints those whose outlying *time* value is the farthest from the linear trends *time/dist* and *time/climb* -- the lack of proper logarithmic transform of the dataset both produces inaccurate models and also tricks the analyst into preferring an over-parametrized model whose effest is just to amplify the importance of outlying value(s).

## Exercise 8

### Text

Apply the `lm.ridge()` function to the `litters` data, using the generalized cross-validation (GCV) criterion to choose the tuning parameter. (GCV is an approximation to cross-validation.)

a. In particular, estimate the coefficients of the model relating `brainwt` to `bodywt` and `lsize` and compare with the results obtained using `lm()`.  
b. Using both ridge and ordinary regression, estimate the mean brain weight when litter size is 10 and body weight is 7. Use the bootstrap, with case-resampling, to compute approximate 95% percentile confidence intervals using each method. Compare with the interval obtained using `predict.lm()`.

### Solution

Let's begin by taking a quick look at the data

```{r daag6_8a, echo=TRUE}
library(DAAG)
library(MASS) #for lm.ridge()

head(litters)
plot(litters)
```

From the basic plot we can immediately tell that
- `brainwt` is positively correlated to `bodywt` and negatively correlated to `lsize`, although the correlation appears to be very noisy;
- There's a significative negative correlation between `bodywt` and `lsize`;


we may expect some problems arising from the collinearity underlined in the second bullet when fitting `brainwt` w.r.t. `bodywt` and `lsize`.

Let's see how the linear model and ridge regression do:

```{r daag6_8b,echo=TRUE}

# fit the vanilla linear model
litters.lm = lm(brainwt~bodywt+lsize, data=litters)
summary(litters.lm)

# fit the ridge linear regression (selecting lambda by GCV)
select(lm.ridge(brainwt~bodywt+lsize, data=litters,
                lambda = seq(0,1,0.001)))
best.lambda = .118
litters.ridge = lm.ridge(brainwt~bodywt+lsize, data=litters,
                         lambda=best.lambda)
litters.ridge


# estimate train MSE for comparing the models

# function that computes the predictions of a lm.ridge() model
predridge <- function(model = litters.ridge, littersdf = litters){
  coeffs = coef(model)
  return(coeffs[1] + coeffs[2] * littersdf$bodywt + coeffs[3] * littersdf$lsize)
}

litters.ridge.residuals <- litters$brainwt - predridge()

print(paste("lm MSE: ",
            sum(litters.lm$residuals**2) / length(litters.lm$residuals)))
print(paste("ridge MSE: ",
            sum(litters.ridge.residuals**2) / length(litters.ridge.residuals)))

```
The coefficients estimated in ridge regression for both predictors are slightly smaller than those fitted by the linear model without regolarization, while the intercept's value increases a bit. 

The difference in MSE is very small, still linear regression appears to perform better on the trained samples, suggesting that our worries for multicollinearity were not solved by ridge regression.

Let's now check our models' prediction on a the novel sample $(7,10)$. We'll also estimate the studentized bootstrap 95% confidence interval for both methods for the sample and compare them with the 95% CI reported by `lm()`:


```{r daag6_8c, echo=TRUE}

# estimate models on the new datapoint
newpoint = data.frame(bodywt=7, lsize=10)

yhat.lm <- predict(litters.lm, newdata = newpoint)
yhat.ridge <- predridge(litters.ridge,newpoint)


# estimate studentized bootstrap CI for lm and lm.ridge at newpoint

n <- nrow(litters)
B = 1000

lm.zb = 1:B
ridge.zb = 1:B

for(b in 1:B){
  idxs = sample (1:n,n, replace = TRUE)
  bootdf = litters[idxs,]
  
  b_lm = lm(brainwt~ bodywt + lsize, data = bootdf)
  yhat.b_lm <- predict(b_lm, newdata = newpoint)
  se.b_lm <- sd(b_lm$fitted.values)
  lm.zb[b] <- (yhat.b_lm - yhat.lm)/se.b_lm
  
  b_ridge = lm.ridge(brainwt~ bodywt + lsize, data = bootdf, lambda = best.lambda)
  yhat.b_ridge = predridge(b_ridge, newpoint)
  se.b_ridge = sd(predridge(b_ridge,bootdf))
  ridge.zb[b] <- (yhat.b_ridge - yhat.ridge)/se.b_ridge
}

se.lm <- sd(litters.lm$fitted.values)
se.ridge <- sd(predridge())

lm.bquants = quantile(lm.zb,probs=c(0.975,0.025))
lm.ci = yhat.lm - se.lm*lm.bquants

ridge.bquants = quantile(ridge.zb,probs=c(0.975,0.025))
ridge.ci = yhat.ridge - se.ridge*ridge.bquants

print("lm estimate + bootstrap 95% CI:")
print(paste(yhat.lm, "(", lm.ci[1],",",lm.ci[2],")"))

print("ridge estimate + bootstrap 95% CI:")
print(paste(yhat.ridge, "(", ridge.ci[1],",",ridge.ci[2],")"))

print("lm() estimate and CI")
predict(litters.lm, newdata = newpoint,interval="confidence", level=.95)

lm.cibase = predict(litters.lm, newdata = newpoint,interval="confidence", level=.95)[2:3]

#plot the three intervals 
plot(as.factor(c("lm+bs","lm+wald","ridge+bs")),c(yhat.lm,yhat.lm,yhat.ridge),       
     ylim=c(.4,.43))

lines(c(1,1),lm.ci, col="red")
lines(c(2,2),lm.cibase, col="red")
lines(c(3,3),ridge.ci, col="red")

```

We can see from the graph that the confidence intervals estimated are very close. Notice that, as expected, the boostrap estimated confidence intervals are asymmetric.

## Exercise 10

### Text
The data frame table.b3 in the MPV package contains data on gas mileage and $11$ other variables for a sample of $32$ automobiles.

(a) Construct a scatterplot of y (mpg) versus x1 (displacement). Is the relationship between these variables non-linear?

### Solution (a)

```{r daag06_10, code = readLines("src/DAAG_06_10_A.R"), echo=TRUE}
```

just by looking on the scatter plot we may suppose a non linear relationship between the output y and the predictor x1: these variables seems to be negative correlated (as x1 increase, y decrease) according to a convex smoothed curve. We may consider some possible transformation.

Not strange points are (yet) detected.

(b) Use the xyplot() function, and x11 (type of transmission) as a group variable. Is a linear model reasonable for these data?

### Solution (b)

```{r daag06_10, code = readLines("src/DAAG_06_10_B.R"), echo=TRUE}
```

just by looking on the plot, we may suppose that the data might be well divided according to the trasmission method (column x11), infact the observation belonging to $x11 = 0$ are in the x1 range $[100, 200]$. While, the others belong to the range $[200, 500]$. Even if the dataset is splitted in two subset, the kind of relationship between y and x1 is still visible, but this time we may suppose a high correlation for the group $x1 = 0$  than the other group.

In the end, according to the split, we may suppose that now is reasonable to consider the two models like a linear models.

Not strange points are (yet) detected for both models.

(c) Fit the model relating y to x1 and x11 which gives two lines having possibly different slopes and intercepts. Check the diagnostics. Are there any influential observations? Are there any influential outliers?

### Solution (c)

```{r daag06_10, code = readLines("src/DAAG_06_10_C.R"), echo=TRUE}
```

According to the summary function for the group x11 = 0, we may make a list of notes:
- residuals: they are spreaded in a quite large range and they seem to be not so symmetrical (more concentrated on the left side of the median).
- x1       : the estimate is the slope of the linear regressor and it's a negative value but quite close to zero (so the slope of the line is not so marked). Moreover, the t statistic is, in module, quite small but the p-value still suggests to reject the null hypothesis agains the estimate.
- R^2 adj  : the variability of the model is explained with the 67% just by considering the x1 predictor, we may do better by looking on other predictors.
- F-stat   : it's value is not so far from 1 but the p-value suggests us to reject the null hypothesis against all the coefficients estimated (without considering the intercept).

In conclusion, we may accept this simple linear regressor, but taking into account that it does not be able to explain very well the model.

While for the group x11 = 1 we have the following observations:
- residuals: they are spreaded in a resonable range and they seem to be symmetrical.
- x1       : the estimate is the slope of the linear regressor and it's a negative value but quite close to zero (so the slope of the line is not so marked). Moreover, the t statistic is, in module,  quite small but the p-value still suggests to reject the null hypothesis agains the estimate.
- R^2 adj  : the variability of the model is explained with the 58% just by considering the x1 predictor, we may do better by looking on other predictors.
- F-stat   : it's value is not so far from 1 but the p-value suggests us to reject the null hypothesis against all the coefficients estimated (without considering the intercept).

In conclusion, we may accept this simple linear regressor, but taking into account that it does not be able to explain very well the model.

For what concern the residuals for the fitted models we may say something more, in particular for group x11 = 0:

- residual vs fitted: the plot suggest us a kind of pattern among the residuals but the observation used to fit the model are not so much and they look quite spread, so we cannot infere on the non-linear relationship among the residuals of the model.
- qq plot           : the residuals seem to be normal distributed even if the point 15 is much far from the straight line (could be an outlier or a point with high level of information).
- scale-location    : the red line suggests us heteroskedasticity for the residuals but, inasmuch the number of observations is small, we cannot accept the heteroskedasticity
- residual vs lvg   : the point 15 is in the critical region of the plot and it means that it has a strong influence on the model, so it could be an outlier or an influencer. 

While for the group x11 = 1:

- residual vs fitted: the plot suggest no clear pattern among the residuals, and we may suppose a non-linear relationship on the residuals.
- qq plot           : the residuals seem to be not so much normal distributed just by looking on the point outside the range [-1, 1], so the normality assumption is quite weak.
- scale-location    : the red line suggests us homoskedasticity for the residuals that might be correct, and we may accept it.
- residual vs lvg   : no one points is in the critical region, even if the point 17  is close to the borderline, it could be a very influence point.

(d) Plot the residuals against the variable x7 (number of transmission speeds), again using x11 as a group variable. Is there anything striking about this plot?

### Solution (d)

```{r daag06_10, code = readLines("src/DAAG_06_10_D.R"), echo=TRUE}
```

The residual plot for the group x11 = 0 shows us that the points are  spread just on three values of x7 (integer values) and seem doesn't  exist a clear pattern among them.

While in the residual plot for the group x11 = 1, the points are all  distributed on the single value of x7, so it might suggest that x7 is not a suitable predictor for our model because we have multiple output for the same value of x7: we may discard it.

## Exercise 11

### Text

### Solution

# Exercises from *DAAG, Chapter 8* 

## Exercise 1

### Text

### Solution

## Exercise 2

### Text

In the data set (an artificial one of 3121 patients, that is similar to a subset of the data analyzed in Stiell et al., 2001) minor.head.injury, obtain a logistic regression model relating clinically.important.brain.injury to other variables. Patients whose risk is sufficiently high will be sent for CT (computed tomography). Using a risk threshold of 0.025 (2.5%), turn the result into a decision rule for use of CT.

### Solution

```{r daag06_10, code = readLines("src/DAAG_08_02.R"), echo=TRUE}
```

### Comments

Almost all of the independent variables are highly significant according to the p-values and the more effective variables have an impact on the output of two times the assumed value of the variable itself (i.e. betahat_i is approximately equals to 2, than y_i = …+ 2 * x_i + …).

The difference between Null model and the current one is quite large so it’s good, in fact 1 - pchisq(1741.6 - 1201.3, 3120 - 3110) = 0 and it means there is evidence against the null hypothesis i.e. to pick the model with just a constant value.

## Exercise 3

### Text

### Solution


## Exercise 6

### Text

### Solution
```{r daag08_06, code = readLines("src/DAAG_08_06.R"), echo=TRUE}
```

### Comments
<!-- TODO -->
Text to be written. Please leave it here!

<!-- LEAVE A NEWLINE AT THE END-OF-FILE! -->
