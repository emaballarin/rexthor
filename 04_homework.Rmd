---
title: "SMDS Homework - Block 4"
author: "P. Morichetti, M. Rispoli, A. Cicchini and E. Ballarin  |  Group 'B'"
date: "27th May 2020"
output:
  html_document:
    theme: darkly
#    highlight: breezedark
    mathjax: default
    self_contained: true
    md_extensions: +autolink_bare_uris
    toc: true
    toc_collapsed: false
    toc_float: true
    toc_depth: 3
    number_sections: false
header-includes:
- \usepackage{color}
- \usepackage{graphicx}
- \usepackage{grffile}
institute: University of Trieste, SISSA, ICTP, University of Udine
graphics: yes
fontsize: 10pt
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/')
```

```{r setup, include=FALSE}
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
```


# Exercises from *DAAG, Chapter 6* 

## Exercise 6

### Text

The following code snippet investigates the consequences of not using a logarithmic transformation for the `nihills` dataset analysis.  

The second model differs from the first in having a $\mathsf{dist} \times \mathsf{climb}$ interaction term, additional to linear terms in $\mathsf{dist}$ and $\mathsf{climb}$.  

(a) Fit the two models:  
`nihills.lm  <- lm(time ~ dist+climb, data=nihills)`  
`nihills2.lm <- lm(time ~ dist+climb+dist:climb, data=nihills)`  
`anova(nihills.lm, nihills2.lm)`  

(b) Using the F-test result, make a tentative choice of model, and proceed to examine diagnostic plots.  
Are there any problematic observations? What happens if these points are removed?  

Refit both of the above models, and check the diagnostics again.


### Solution

```{r daag_06_06, code = readLines("src/DAAG_06_06.R"), echo=TRUE}
```

### Comments

After having fit both *Model 1* (i.e. the linear model that tries to explain record `time` for male athletes from the `dist`ance and `climb` variables, without considering the interaction term) and *Model 2* (the one considering also interaction between `dist` and `climb`) on the entire, un-transformed, `nihills` dataset, we can look at *Fisher-test ANOVA* for model selection, being *Model 1* nested inside *Model 2*.  

With a value of $F = 72.406$ and a *p-value* $<10^{-7}$ *Model 2* is evidently (according to the *F-test*) the model of choice in such setting.  

Upon inspection, however, it appears clear that the relatively poor behaviour of *Model 1* as opposed to *Model 2* is strongly determined by the influence of few data-points (one, mainly!).  

In fact, considering for the moment *Model 1*:

- From the *Residuals vs Fitted* graph, it appears clearly that tracks *Seven Sevens* and *Annalong Horseshoe* have both highest residuals (though of opposite sign) and highest fitted values. This evidence of heteroscedasticity is further confirmed by the *Scale-Location* plot.  
- From the *Residuals vs Leverage* plot we know that the *Seven Sevens* track is also a potential influential point. Such findings, together with the observation that *Seven Sevens* is the only point in the dataset with such comparably high fitted value from the model, make it a highly likely *over*-influential point in the fitted model.

Now, looking at *Model 2* diagnostic plots, we can see that:
- The *Seven Sevens* track has become one of the datapoints with the smallest residuals, but with a fitted value comparable with that previously obtained.  
- Such datapoint still exhibits high leverage and uniqueness among the datapoints with such high fitted value: it is still a candidate to be an *over*-influential point.  

We can therefore conclude that the additional interaction term (and associated degree of freedom for the model) mainly serves the purpose of correcting the model behaviour for taking into account the appearantly *deviant* behaviour of *Seven Sevens* datapoint in *Model 1*.  

For that reason, we try to re-fit the model on the (still, untransformed) `nihills` dataset, after removal of the *Seven Sevens* datapoint.  

This time, *Fisher-test ANOVA* suggests to avoid the additional interaction term and to accept the simpler model, proving our previous assumption as probably correct.  

Now, at the price of some additional residuals-non-normality even at central quantiles, and without solving the still present problem of heteroscedasticity, the model is less *over*-determined by the influence of single-datapoints.  

Still, the behaviour previously observed for the *Seven Sevens* datapoint can now be found -- though with a smaller effect overall -- in the behaviour of the *Annalong Horseshoe* datapoint. We still observe high (and unique) fitted value, a residual among the highest and a high Cook distance from zero.   

Repeating the fit on the dataset additionally devoided of the *Annalong Horseshoe* datapoint, previously-evidenced conclusions are even more amplified and confirmed: the final model (still, the simpler one) obtained this way is no longer *over*-determined by single datapoints and more balanced overall w.r.t. high-residual, high-leverage points.  

The interesting result to be noted with such regard is that -- being the *Seven Sevens* and *Annalong Horseshoe* datapoints those whose outlying associated $\mathsf{time}$ value is the farthest from the linear trends *time/dist* and *time/climb* -- the lack of proper logarithmic transformation of the dataset both produces generally inaccurate models and also may trick the analyst into preferring an over-parametrized model whose effect is just that to amplify the importance of such *outlying* value(s).

## Exercise 8

### Text

Apply the `lm.ridge()` function to the `litters` data, using the generalized cross-validation (GCV) criterion to choose the tuning parameter. (GCV is an approximation to cross-validation.)

a. In particular, estimate the coefficients of the model relating `brainwt` to `bodywt` and `lsize` and compare with the results obtained using `lm()`.  
b. Using both ridge and ordinary regression, estimate the mean brain weight when litter size is 10 and body weight is 7. Use the bootstrap, with case-resampling, to compute approximate 95% percentile confidence intervals using each method. Compare with the interval obtained using `predict.lm()`.

### Solution

Let's begin by taking a quick look at the data

```{r daag6_8a, echo=TRUE}
library(DAAG)
library(MASS) #for lm.ridge()

head(litters)
plot(litters)
```

From the basic plot we can immediately tell that
- `brainwt` is positively correlated to `bodywt` and negatively correlated to `lsize`, although the correlation appears to be very noisy;
- There's a significative negative correlation between `bodywt` and `lsize`;


we may expect some problems arising from the collinearity underlined in the second bullet when fitting `brainwt` w.r.t. `bodywt` and `lsize`.

Let's see how the linear model and ridge regression do:

```{r daag6_8b,echo=TRUE}

# fit the vanilla linear model
litters.lm = lm(brainwt~bodywt+lsize, data=litters)
summary(litters.lm)

# fit the ridge linear regression (selecting lambda by GCV)
select(lm.ridge(brainwt~bodywt+lsize, data=litters,
                lambda = seq(0,1,0.001)))
best.lambda = .118
litters.ridge = lm.ridge(brainwt~bodywt+lsize, data=litters,
                         lambda=best.lambda)
litters.ridge


# estimate train MSE for comparing the models

# function that computes the predictions of a lm.ridge() model
predridge <- function(model = litters.ridge, littersdf = litters){
  coeffs = coef(model)
  return(coeffs[1] + coeffs[2] * littersdf$bodywt + coeffs[3] * littersdf$lsize)
}

litters.ridge.residuals <- litters$brainwt - predridge()

print(paste("lm MSE: ",
            sum(litters.lm$residuals**2) / length(litters.lm$residuals)))
print(paste("ridge MSE: ",
            sum(litters.ridge.residuals**2) / length(litters.ridge.residuals)))

```
The coefficients estimated in ridge regression for both predictors are slightly smaller than those fitted by the linear model without regolarization, while the intercept's value increases a bit. 

The difference in MSE is very small, still linear regression appears to perform better on the trained samples, suggesting that our worries for multicollinearity were not solved by ridge regression.

Let's now check our models' prediction on a the novel sample $(7,10)$. We'll also estimate the studentized bootstrap 95% confidence interval for both methods for the sample and compare them with the 95% CI reported by `lm()`:


```{r daag6_8c, echo=TRUE}

# estimate models on the new datapoint
newpoint = data.frame(bodywt=7, lsize=10)

yhat.lm <- predict(litters.lm, newdata = newpoint)
yhat.ridge <- predridge(litters.ridge,newpoint)


# estimate studentized bootstrap CI for lm and lm.ridge at newpoint

n <- nrow(litters)
B = 1000

lm.zb = 1:B
ridge.zb = 1:B

for(b in 1:B){
  idxs = sample (1:n,n, replace = TRUE)
  bootdf = litters[idxs,]
  
  b_lm = lm(brainwt~ bodywt + lsize, data = bootdf)
  yhat.b_lm <- predict(b_lm, newdata = newpoint)
  se.b_lm <- sd(b_lm$fitted.values)
  lm.zb[b] <- (yhat.b_lm - yhat.lm)/se.b_lm
  
  b_ridge = lm.ridge(brainwt~ bodywt + lsize, data = bootdf, lambda = best.lambda)
  yhat.b_ridge = predridge(b_ridge, newpoint)
  se.b_ridge = sd(predridge(b_ridge,bootdf))
  ridge.zb[b] <- (yhat.b_ridge - yhat.ridge)/se.b_ridge
}

se.lm <- sd(litters.lm$fitted.values)
se.ridge <- sd(predridge())

lm.bquants = quantile(lm.zb,probs=c(0.975,0.025))
lm.ci = yhat.lm - se.lm*lm.bquants

ridge.bquants = quantile(ridge.zb,probs=c(0.975,0.025))
ridge.ci = yhat.ridge - se.ridge*ridge.bquants

print("lm estimate + bootstrap 95% CI:")
print(paste(yhat.lm, "(", lm.ci[1],",",lm.ci[2],")"))

print("ridge estimate + bootstrap 95% CI:")
print(paste(yhat.ridge, "(", ridge.ci[1],",",ridge.ci[2],")"))

print("lm() estimate and CI")
predict(litters.lm, newdata = newpoint,interval="confidence", level=.95)

lm.cibase = predict(litters.lm, newdata = newpoint,interval="confidence", level=.95)[2:3]

#plot the three intervals 
plot(as.factor(c("lm+bs","lm+wald","ridge+bs")),c(yhat.lm,yhat.lm,yhat.ridge),
    ylim=c(.4,.43))

lines(c(1,1),lm.ci, col="red")
lines(c(2,2),lm.cibase, col="red")
lines(c(3,3),ridge.ci, col="red")

```

We can see from the graph that the confidence intervals estimated are very close. Notice that, as expected, the boostrap estimated confidence intervals are asymmetric.

## Exercise 10

### Text
The data frame table.b3 in the MPV package contains data on gas mileage and $11$ other variables for a sample of $32$ automobiles.

### Question a

Construct a scatterplot of y (mpg) versus x1 (displacement). Is the relationship between these variables non-linear?

#### Solution

```{r}
library(MPV)
library(lattice)

plot_fit <- function(fitted_model, x, y, xlab, subtitle){
  plot(x, y, main = "fitted model", xlab = xlab, sub = subtitle)
  lines(x, as.vector(fitted_model$fitted.values), col ="red")
  text(13,3, "y = b0+b1*x", col="red")
}
```


```{r daag06_10a, code = readLines("src/DAAG_06_10_A.R"), echo=TRUE}
```

just by looking on the scatter plot we may suppose a non linear relationship between the output y and the predictor x1: these variables seem to be negative related (as x1 increase, y decrease) according to a convex smoothed curve. We may consider some possible transformation.

Not strange points are (yet) detected.

### Question b

Use the xyplot() function, and x11 (type of transmission) as a group variable. Is a linear model reasonable for these data?

#### Solution

```{r daag06_10b, code = readLines("src/DAAG_06_10_B.R"), echo=TRUE}
```

just by looking on the plot, we may suppose that the data might be well divided according to the trasmission method (column x11), in fact the observation belonging to $x11 = 0$ are in the x1 range $[100, 200]$. While, the others belong to the range $[200, 500]$. Even if the dataset is splitted in two subset, the kind of relationship between y and x1 is still visible, but this time we may assume a strongest relation among data for the group $x1 = 0$  than the other group.

In the end, according to the split, it is reasonable to consider the two models like a linear models.

Not strange points are (yet) detected for both models.

### Question c

Fit the model relating y to x1 and x11 which gives two lines having possibly different slopes and intercepts. Check the diagnostics. Are there any influential observations? Are there any influential outliers?

#### Solution

```{r daag06_10c, code = readLines("src/DAAG_06_10_C.R"), echo=TRUE}
```

According to the summary function for the group x11 = 0, we may say something: 
* residuals: they are spreaded in a quite large range and they seem to be not so symmetrical (more concentrated on the left side of the median).

* x1       : the estimate is the slope of the linear regressor and it's a negative value but quite close to zero (so the slope of the line is not so marked). Moreover, the t statistic is, in module, quite small but the p-value still suggests to reject the null hypothesis against the estimate.

* R^2 adj  : the variability of the model is explained with the 67% just by considering the x1 predictor, we may do better by looking on other predictors.

* F-stat   : it's value is not so far from 1 but the p-value suggests us to reject the null hypothesis against all the coefficients estimated (without considering the intercept).

In conclusion, we may accept this simple linear regressor, but taking into account that it does not be able to explain very well the model.

While for the group x11 = 1 we have the following observations:

* residuals: they are spreaded in a resonable range and they seem to be symmetrical.

* x1       : the estimate is the slope of the linear regressor and it's a negative value but quite close to zero (so the slope of the line is not so marked). Moreover, the t statistic is, in module,  quite small but the p-value still suggests to reject the null hypothesis against the estimate.

* R^2 adj  : the variability of the model is explained with the 58% just by considering the x1 predictor, we may do better by looking on other predictors.

* F-stat   : it's value is not so far from 1 but the p-value suggests us to reject the null hypothesis against all the coefficients estimated (without considering the intercept).

In conclusion, we may accept this simple linear regressor, but taking into account that it does not be able to explain very well the model.

For what concern the residuals for the fitted models we may say something more, in particular for group x11 = 0:

* residual vs fitted: the plot suggest us a kind of pattern among the residuals but the observation used to fit the model are not so much and they look quite spread, so we cannot infere on the non-linear relationship among the residuals of the model.

* qq plot           : the residuals seem to be normal distributed even if the point 12 is much far from the straight line (could be an outlier or a point with high level of information).

* scale-location    : the red line suggests us homoscedasticity for the residuals but, inasmuch the number of observations is small, we cannot accept the homoscedasticity.

* residual vs lvg   : the point 5 is in the critical region of the plot and it means that it has a strong influence on the model, so it could be an outlier or an influencer. 

The point 5 seems to be both an outlier and an influencer because its cook distance is very large, moreover its residual is significantly far from zero respect all the others, and in the end it is the only point fitted in the first portion of the model and this fact associates a certain relevance to this point.

While for the group x11 = 1:

* residual vs fitted: the plot suggest no clear pattern among the residuals, and we may suppose a non-linear relationship on the residuals.

* qq plot           : the residuals seem to be not so much normal distributed just by looking on the point outside the range [-1, 1], so the normality assumption is quite weak.

* scale-location    : the red line suggests us homoskedasticity for the residuals that might be correct, and we may accept it.

* residual vs lvg   : no one points is in the critical region, even if the point 17  is close to the borderline, it could be a very influence point.

### Question d

Plot the residuals against the variable x7 (number of transmission speeds), again using x11 as a group variable. Is there anything striking about this plot?

#### Solution

```{r daag06_10d, code = readLines("src/DAAG_06_10_D.R"), echo=TRUE}
```

The residual plot for the group x11 = 0 shows us that the points are  spread just on three values of x7 (integer values) and seem doesn't  exist a clear pattern among them.

While in the residual plot for the group x11 = 1, the points are all  distributed on the single value of x7, so it might suggest that x7 is not a suitable predictor for our model because we have multiple output for the same value of x7: we may discard it.

## Exercise 11

### Text

### Solution
<!-- ANDREA! -->

# Exercises from *DAAG, Chapter 8* 

## Exercise 1

### Text

It is given the numbers of occasions when inhibition (i.e., no flow of current across a membrane) occurred within $120$s, for different concentrations of the protein *peptide-C* (data are used with the permission of Claudia Haarmann, who obtained these data in the course of her PhD research). The outcome `yes` implies that inhibition has occurred.  
Use logistic regression to model the probability of inhibition as a function of protein concentration.


### Solution
```{r daag08_01, code = readLines("src/DAAG_08_01.R"), echo=TRUE}
```

### Comments

Following an *orthodox interpretation* of the exercise text, we are required to fit a *logistic model* that tries to explain or peredict the probability of inhibition as a function of concentration -- assming that the counts shown in the datatable refer to different, independent $120\text{s}$-long observations.  

Upon visual analysis of the scatterplot of the data, it appears that a global trend in the data is hard to be noticed, and both *up-* and *down-**swinging* trends are present as the concentration increases.  

The best we can do in this case os to fit a *logistic regression* model on untransformed data, directly for the continuous variable that computes the probability of inhibition, using the total number of *yes/no* observations per concentration as weights.  

The result is acceptable, but still corrupted by noise.  


## Exercise 2

### Text

In the data set (an artificial one of 3121 patients, that is similar to a subset of the data analyzed in Stiell et al., 2001) minor.head.injury, obtain a logistic regression model relating clinically.important.brain.injury to other variables. Patients whose risk is sufficiently high will be sent for CT (computed tomography). Using a risk threshold of 0.025 (2.5%), turn the result into a decision rule for use of CT.

### Solution

```{r daag08_02, code = readLines("src/DAAG_08_02.R"), echo=TRUE}
```

### Comments

Almost all of the independent variables are highly significant according to the p-values and the more effective variables have an impact on the output of two times the assumed value of the variable itself (i.e. betahat_i is approximately equals to 2, than y_i = …+ 2 * x_i + …).

The difference between Null model and the current one is quite large so it’s good, in fact 1 - pchisq(1741.6 - 1201.3, 3120 - 3110) = 0 and it means there is evidence against the null hypothesis i.e. to pick the model with just a constant value.

In the end, it is provided a clear table to summarize the estimated coefficients and the patients information; in addition, another table is provided to show the risk level for each patients and if it is a good idea to sent them for a computed tomography check (CT).

## Exercise 3

### Text

Consider again the $\mathsf{moths}$ dataset of *Section 8.4*.  
(a) What happens to the standard error estimates when the *poisson* family is used in $\mathsf{glm()}$ instead of the *quasipoisson* family?  
(b) Analyze the *P moths*, in the same way as the *A moths* were analyzed. Comment on the effect of transect length.  


### Solution
```{r daag08_03, code = readLines("src/DAAG_08_03.R"), echo=TRUE}
```

### Comments

(a) As we can see from the printed results -- and as also noted in *Exercise DAAG 8.6* -- standard error intervals estimated from a *Poisson regression* model are narrower w.r.t. *quasi-poisson* ones. This can be explained as a positive dispersion coefficient (as the one estimated in this case for the *quasi-Poisson* model inflates error bounds).

(b) By omitting the portions of data analysis directly determined by the observed $0$-count for the *Bank* habitat (in the case of *A moths*), the analysis is pretty compact and is contained in the snipped above.  

With a difference w.r.t. the analysis shown in the book, the reference level among different habitats has been chosen in order to maximize overall significance for the coefficients in the linear model leading to the *(quasi-)Poisson* one. Since no clear dominance exists in such choice, the closest to hypotetical convex Pareto front has been chosen. Regardless of such criterion, all observations contained in the following are invariant to such choice.  

From the summary of the fitted GLM we can observe that the $log(\text{length})$ term has a significant coefficient ($p < 0.004$). The effect of transect length on moth count is therefore relevant (definitely more relevant in comparison to the *A moths*). This can show that -- w.r.t. *A moths*, *P moths* enjoy longer flights and/or pass significantly many (more) times across transects.



## Exercise 6

### Text

The function $\mathsf{poissonsim()}$ allows for experimentation with *Poisson regression*.  
In particular, $\mathsf{poissonsim()}$ can be used to simulate Poisson responses with *log-rates* equal to $a + bx$, where $a$ and $b$ are fixed values by default.  

(a) Simulate $100$ Poisson responses using the model $log( \lambda) = 2 − 4x$ for $x = 0, 0.01, 0.02,\dots, 1.0$.  
Fit a Poisson regression model to these data, and compare the estimated coefficients with the true coefficients. How well does the estimated model predict future observations?  

(b) Simulate $100$ Poisson responses using the model $log( \lambda) = 2 − bx$ where $b$ is normally distributed with mean $4$ and standard deviation $5$. [Use the argument $\mathsf{slope.sd=5}$ in the $\mathsf{poissonsim()}$ function.] How do the results using the poisson and quasipoisson families differ?

### Solution
```{r daag08_06, code = readLines("src/DAAG_08_06.R"), echo=TRUE}
```

### Comments

(a) Preliminarily to a more *in-depth* analysis, it is worth noting that -- w.r.t. to the point we are currently answering to -- we are performing a *Poisson regression* model to data generated according to a *Poisson regression* generative model.  

Far from being *that* obvious, this allows us to preliminarily state that what we are trying to accomplish is *a posteriori* (information-theoretically) the most efficient fitting procedure to predict the expected counts from the only available predictor of the synthetic phenomenon we are dealing with.  

A more precise consistency check involves the analysis of the *standard error* estimates (obtained by the means of the integrated fitting routine $\mathsf{glm}$) and *approximated-normal confidence intervals* obtained via *profile likelihood estimation* thanks to the `MASS::confint()` function.

As we can see from the results shown above, the estimated coefficients are always included in the $95\%$ symmetric C.I.s around the estimate. This is also true for the $\text{estimate}\ \pm\ SE$ interval for the slope coefficient and sometimes also for the intercept. In cases it is not, anyway, the difference is always of minor entity.  

To analyze the robustness of the fitted model for $x \rightarrow +\infty$ (i.e. *future values*) it is possible to study the (estimated) rate $\hat{\lambda}$ of the (estimated) Poisson distribution from which we assume our counts to be sampled from.  

In fact, $\hat{\lambda} = e^{\hat{a} + x\hat{b}}$ and it is sufficient to consider that $\hat{a} > 0$ and $\hat{b} < 0$. Furthermore, as $x \rightarrow +\infty, \ \hat{\lambda} \rightarrow 0$ and $E_{pois}[\text{counts}] \propto \lambda$.  

From that -- or also via *interval-bound propagation* -- it is possible to show that, as long as the previous inequalities hold true, the difference in predicted counts converges to zero as $x \rightarrow +\infty$ regardless of the specific value of the estimated parameters $\hat{a} > 0$ and $\hat{b} < 0$. This makes *future* estimations robust to stochastic noise.  


(b) As far as the second phenomenon and model are concerned, the following are the similarities and differences among the *Poisson-family* and *Quasi-Poisson family* regressions.  

- As expected, point-estimates for the *(quasi-)Poisson regression* coefficients are the same in both cases, since *Quasi-Poisson* regression model is the *quasi-likelihood* model associated to the *Poisson* one with non-locked dispersion;

- Both the confidence intervals and the standard errors shown for the *Quasi-Poisson* model are broader w.r.t. the *Poisson* ones;

- The *AIC* is not shown for the *Quasi-Poisson* model. Settling a seemingly popular debate, such behaviour is a design choice of *R Core Team* for any *quasi-likelihood* model fitted using the $\mathsf{glm}$ function.  

Overall, it can be said that *Quasi-Poisson* regression accounts for (in this case) increased dispersion of the data via broadening of C.I.s.

<!-- LEAVE A NEWLINE AT THE END-OF-FILE! -->
